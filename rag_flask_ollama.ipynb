{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c59227f",
   "metadata": {},
   "source": [
    "# RAG_Flask_Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1412c",
   "metadata": {},
   "source": [
    "This notebook will demonstrate a step-by-step procedure of creating a Retrieval-Augmented Generation (RAG) pipeline using LangChain and locally deploying the LLM application using Flask and Ollama. The data set used in this notebook consist of pdf files of research articles on the topic of AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338667b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ohjin\\anaconda3\\envs\\rag_flask_ollama\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version information\n",
      "python: 3.11.13 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:03:15) [MSC v.1929 64 bit (AMD64)]\n",
      "numpy: 1.24.3\n",
      "langchain: 0.2.14\n",
      "langchain_community: 0.2.12\n",
      "sentence_transformers: 3.0.1\n",
      "pypdf: 4.3.1\n",
      "langchain_core: 0.2.43\n",
      "langchain_openai: 0.1.25\n",
      "python-dotenv: 1.1.1\n",
      "ollama: 0.5.3\n"
     ]
    }
   ],
   "source": [
    "#Import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import langchain\n",
    "import langchain_community\n",
    "import langchain_core\n",
    "import langchain_openai\n",
    "import sentence_transformers\n",
    "import pypdf\n",
    "import dotenv\n",
    "import importlib.metadata\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "print('Version information')\n",
    "\n",
    "print('python: {}'.format(sys.version))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('langchain: {}'.format(langchain.__version__))\n",
    "print('langchain_community: {}'.format(langchain_community.__version__))\n",
    "print('sentence_transformers: {}'.format(sentence_transformers.__version__))\n",
    "print('pypdf: {}'.format(pypdf.__version__))\n",
    "print('langchain_core: {}'.format(langchain_core.__version__))\n",
    "print('langchain_openai: {}'.format(importlib.metadata.version(\"langchain-openai\")))\n",
    "print('python-dotenv: {}'.format(importlib.metadata.version(\"python-dotenv\")))\n",
    "print('ollama: {}'.format(importlib.metadata.version(\"ollama\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787aa45",
   "metadata": {},
   "source": [
    "## 1) Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faef326",
   "metadata": {},
   "source": [
    "State-of-the-art Large Language Models (LLM's) are trained on an enormous corpus of textual data (taken from webpages, articles, books etc.), and they store a wide range of general knowledge in their parameters. While they are able to perform well on tasks that require general knowledge, they tend to struggle on tasks that require information that wasn't present in the training data. For instance, LLM's may struggle on tasks that require knowledge of domain-specific information, or even up-to-date information.\n",
    "\n",
    "It is very important to overcome this problem, because it is undesirable to get a non-answer from the LLM, and potentially even dangerous if the LLM begins to hallucinate (i.e ramble on with an answer that seems believable but is factually inaccurate). Therefore, it is essential to bridge the gap between the LLM's general knowledge, and other domain-specific or up-to-date information in order to help the LLM generate responses that are contextual and factually accurate, while reducing the chances of hallucinations.\n",
    "\n",
    "There are two effective ways of accomplishing this:\n",
    "1. <strong>Fine-tuning the LLM on the domain-specific/proprietary/new data</strong>:\n",
    "    <ul>\n",
    "        <li>By fine-tuning the model, it can be made suitable for the task.</li>\n",
    "        <li>However, this comes with some limitations. It is compute-intensive, expensive and not agile (it's not realistic to fine-tune an LLM with the new data coming in everyday)</li>\n",
    "    </ul>\n",
    "2. <strong>Retrieval Augmented Generation (RAG)</strong>:\n",
    "    <ul>\n",
    "        <li><a href='https://arxiv.org/abs/2005.11401'>This technique</a> provides the LLM with contextual information from an external knowledge source that can be updated more easily.</li>\n",
    "        <li>It allows the LLM to generate more contextual, and factually accurate responses by allowing it to dynamically access information from an external knowledge source.</li>\n",
    "    </ul>\n",
    "\n",
    "<center><img src=\"data/images/rag-architecture.png\" alt=\"drawing\" width=\"700\" align='center'></center>\n",
    "<center>Basic RAG Architecture</center>\n",
    "\n",
    "\n",
    "The RAG pipeline consists of the following steps:\n",
    "<ol>\n",
    "    <li><strong>Retrieval</strong>: The user's query is used to retrieve the relevant contextual information from the external knowledge source. The external knowledge source is a vector store that contains the embeddings of the documents that contain the proprietary/domain-specific data. The user query is embedded into the same vector space as these documents, and a similarity search is performed in this embedding space to retrieve the documents that are most similar to the user's query. These retrieved documents make up the context that the LLM will consider in order to produce factually accurate responses. </li>\n",
    "    <li><strong>Augmentation</strong>: The user's query is augmented with the retrieved contextual information to form the prompt. The prompt will also usually include instructions to the LLM for performing the task. There is an entire sub-field known as Prompt Engineering, that is dedicated to fine-tuning the prompt so as to get the best possible response from the LLM, and you will get some experience with it in <strong>7.2</strong> while implementing the RAG Chain. </li>\n",
    "    <li><strong>Generation</strong>: The constructed prompt, including the instructions to the LLM, the user query and the retrieved context, is fed to the LLM to generate a response that is comprehensible and factually correct.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601b03f",
   "metadata": {},
   "source": [
    "## 1.1) Implementing the Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d547bba",
   "metadata": {},
   "source": [
    "In the first step, the Retriever for the RAG Chain will be implemented. The documents serving as the basis of the external knowledge source will be split into smaller chunks to be used with the vector database. Lastly, the Retriever object will be created to be used with the RAG Chain to retrieve relevant document chunks as additional contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92345806",
   "metadata": {},
   "source": [
    "### 1.1.1) Loading and Pre-processing data: Document Loaders and Text Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af00cb82",
   "metadata": {},
   "source": [
    "Langchain provides <a href='https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/'>several classes</a> to load data into Document objects. There are classes to load data from HTML files, PDF files, JSON files, CSV files, file directories etc. Here, Langchain's <a href='https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/#pypdf-directory'>PyPDFDirectoryLoader</a> will be used to load PDF's from a directory. A Document object is a dictionary that stores the text and metadata about each document.\n",
    "\n",
    "Once the data has been loaded into Document(s), it's common to split the documents into smaller chunks. This is done because when a user inputs a query to the system, the retriever will return the most relevant documents, which will be augmented to the prompt that is sent to the LLM. There are limits to the length of this prompt, since it must fit into the LLM's context window. Therefore, it's common to split documents into smaller chunks, so that only the retrieved relevant chunks are inserted into the prompt. Langchain offers several <a href='https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/'>TextSplitters</a> to perform the document splitting. RecursiveCharacterTextSplitter, which is a way to split a document into several chunks in a manner such that related pieces of text are kept together in a chunk, will be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f4fb04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Tests for Loading and Splitting Documents \n",
      "\n",
      "Your load documents works as expected: True\n",
      "Your split documents works as expected: True\n"
     ]
    }
   ],
   "source": [
    "from retriever import Retriever\n",
    "from local_tests.retriever_test import Retriever_Test\n",
    "\n",
    "local_test = Retriever_Test()\n",
    "stu_retriever = Retriever()\n",
    "num_chunks_to_query = 2\n",
    "\n",
    "print('Local Tests for Loading and Splitting Documents \\n')\n",
    "\n",
    "# Local test for load documents\n",
    "output_documents = stu_retriever.loadDocuments(data_dir='./data/papers/')\n",
    "load_test = (len(output_documents) == local_test.load_documents_len)\n",
    "print('Your load documents works as expected:', load_test)\n",
    "\n",
    "# Local test for split documents\n",
    "output_chunks = stu_retriever.splitDocuments(output_documents, chunk_size=700, chunk_overlap=50)\n",
    "split_test = (len(output_chunks) == local_test.split_documents_len)\n",
    "print('Your split documents works as expected:', split_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c0ab343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents type: <class 'list'>\n",
      "documents[0] type: <class 'langchain_core.documents.base.Document'>\n",
      "documents length: 104\n",
      "\n",
      "Content of documents[0]:\n",
      " page_content='Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023' metadata={'source': 'data\\\\papers\\\\attention.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF documents\n",
    "from retriever import Retriever\n",
    "retriever = Retriever()\n",
    "documents = retriever.loadDocuments(data_dir='./data/papers/')\n",
    "\n",
    "print('documents type:', type(documents))           # python list\n",
    "print('documents[0] type:', type(documents[0]))     # Document object\n",
    "print('documents length:', len(documents))          # each page is loaded as a separate document (104 pages -> 104 documents)\n",
    "\n",
    "print('\\nContent of documents[0]:\\n', documents[0]) # the first document object, containing 'page_content' and 'metadata' fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59310dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document_chunks type: <class 'list'>\n",
      "document_chunks[0] type: <class 'langchain_core.documents.base.Document'>\n",
      "document_chunks length: 701\n",
      "\n",
      "\n",
      " page_content='Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.comNoam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.comNiki Parmar∗\n",
      "Google Research\n",
      "nikip@google.comJakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.comAidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.eduŁukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or' metadata={'source': 'data\\\\papers\\\\attention.pdf', 'page': 0}\n",
      "\n",
      "\n",
      " page_content='convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including' metadata={'source': 'data\\\\papers\\\\attention.pdf', 'page': 0}\n",
      "\n",
      "\n",
      " page_content='ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and' metadata={'source': 'data\\\\papers\\\\attention.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# Split the documents into smaller chunks\n",
    "document_chunks = retriever.splitDocuments(documents)\n",
    "\n",
    "print('document_chunks type:', type(document_chunks)) # python list\n",
    "print('document_chunks[0] type:', type(document_chunks[0])) # Document object\n",
    "print('document_chunks length:', len(document_chunks)) # you should observe that each document (corresponding to a page in a PDF) has been split into several chunks\n",
    "\n",
    "for chunk in document_chunks[:3]:   # displaying the first 3 chunks\n",
    "    print('\\n\\n', chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9dde4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before split, there were 104 documents, with average size equal to 4149.2307692307695.\n",
      "After split, there were 701 documents (chunks), with average size equal to 624.5235378031384.\n"
     ]
    }
   ],
   "source": [
    "# Analysis of chunk size\n",
    "def compute_avg_chunk_size(chunks):\n",
    "    return sum([len(chunk.page_content) for chunk in chunks])/len(chunks)\n",
    "\n",
    "print(f'Before split, there were {len(documents)} documents, with average size equal to {compute_avg_chunk_size(documents)}.')\n",
    "print(f'After split, there were {len(document_chunks)} documents (chunks), with average size equal to {compute_avg_chunk_size(document_chunks)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25cb2d",
   "metadata": {},
   "source": [
    "### 1.1.2) Creating the Vector Database and Retrieval System: Embedding models and Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc6911b",
   "metadata": {},
   "source": [
    "<a href='https://python.langchain.com/v0.2/docs/integrations/text_embedding/'>Langchain provides several embedding models</a>. An embedding model converts a piece of natural language (e.g. a token) into a vector embedding. The next steps will involve using Huggingface's BGE Embedding models, which are the one of the best open-source embedding models <a href='https://python.langchain.com/v0.2/docs/integrations/text_embedding/bge_huggingface/'>(according to Langchain)</a>. The model used (BAAI/bge-small-en-v1.5) has 384-dimensional embedding vectors.\n",
    "\n",
    "After obtaining the embedding model, vectorstore needs to be created by embedding all of the document chunks into the vector space. A retriever will be used to retrieve the document chunks from the vectorstore that are most similar to the user's query using efficient similarity search algorithms. Langchain offers <a href='https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/'>several vectorstores</a>, and Facebook's AI Similarity Search (FAISS) will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b11722e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Tests for Retriever \n",
      "\n",
      "Your retriever embeddings returns the expected shape: True\n",
      "Your retriever returns the expected number of chunks: True\n",
      "Your retriever returns chunks from the expected relevant documents: True\n"
     ]
    }
   ],
   "source": [
    "from retriever import Retriever\n",
    "from local_tests.retriever_test import Retriever_Test\n",
    "\n",
    "local_test = Retriever_Test()\n",
    "stu_retriever = Retriever()\n",
    "num_chunks_to_query = 2\n",
    "\n",
    "print('Local Tests for Retriever \\n')   # ensure that the local tests for the loading and splitting documents pass\n",
    "\n",
    "output_documents = stu_retriever.loadDocuments(data_dir='./data/papers/')\n",
    "output_chunks = stu_retriever.splitDocuments(output_documents, chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Local test for retriever embeddings\n",
    "output_retrieval_system = stu_retriever.createRetriever(output_chunks, num_chunks_to_return=num_chunks_to_query)\n",
    "student_embedding_model = stu_retriever.huggingface_embeddings\n",
    "output_embedding = np.array(student_embedding_model.embed_query(output_chunks[0].page_content))\n",
    "embedding_shape = output_embedding.shape[0]\n",
    "\n",
    "if (student_embedding_model.model_name == local_test.model_name):\n",
    "    embedding_shape_test = (embedding_shape == local_test.embedding_size)\n",
    "    print('Your retriever embeddings returns the expected shape:', embedding_shape_test)\n",
    "else:\n",
    "    print('You are free to choose the embedding model to use for the best results (provided it works with Gradescope),')\n",
    "    print(f'but we can only locally test the embedding shape (no value testing) if using the {local_test.model_name} model')\n",
    "    print('and the RecursiveCharacterTextSplitter.')\n",
    "\n",
    "# Local test for chunk relevance\n",
    "output_retrieved_chunks = output_retrieval_system.invoke(local_test.relevance_prompt)\n",
    "returned_count_test = (len(output_retrieved_chunks) == num_chunks_to_query)\n",
    "relevance_test = True\n",
    "for chunk in output_retrieved_chunks:\n",
    "    if local_test.relevant_pdf not in chunk.metadata['source']:\n",
    "        relevance_test = False\n",
    "print('Your retriever returns the expected number of chunks:', returned_count_test)\n",
    "print('Your retriever returns chunks from the expected relevant documents:', relevance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57d47f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the retriever\n",
    "retrieval_system = retriever.createRetriever(document_chunks)\n",
    "embedding_model = retriever.huggingface_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af441221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the embedding:  (384,)\n",
      "Sample embedding of a document chunk:  [-4.52586636e-02 -4.26387973e-03 -4.41800393e-02 -2.71606948e-02\n",
      " -3.99877205e-02  3.49567235e-02 -3.55790905e-03  1.92805864e-02\n",
      "  6.00379892e-02 -8.17402080e-03  2.22530942e-02 -1.37564540e-02\n",
      "  4.14049439e-02  3.60869430e-02  1.50960404e-02 -2.18757540e-02\n",
      " -5.24056070e-02  3.74929532e-02  9.44254268e-03 -4.80045285e-03\n",
      "  6.43263608e-02 -4.49812412e-02 -7.86372460e-03  1.68564124e-03\n",
      "  2.98901144e-02  4.22352692e-03 -7.87110534e-03 -2.22316664e-02\n",
      " -2.05486305e-02 -2.44379282e-01  1.11250076e-02 -3.14529017e-02\n",
      "  5.13514206e-02  2.52507143e-02 -1.31557165e-02 -2.39112619e-02\n",
      " -5.89635931e-02 -2.62947343e-02 -6.48454800e-02  3.48604023e-02\n",
      "  1.73388626e-02  2.02526003e-02  1.45917793e-03 -1.66223999e-02\n",
      " -1.07647590e-02 -2.87664086e-02 -4.01396714e-02 -4.46370542e-02\n",
      " -4.39290777e-02 -3.29201715e-03 -5.14779538e-02  8.32557492e-03\n",
      " -4.71756868e-02  8.11673328e-02  2.99861450e-02 -3.70062049e-03\n",
      "  3.59496996e-02  2.83317249e-02  2.92633735e-02 -8.37651175e-03\n",
      "  2.10136157e-02  4.45884950e-02 -1.53838083e-01  8.13025050e-03\n",
      "  6.65076524e-02  4.72061373e-02 -7.80448224e-03  5.36702611e-02\n",
      " -6.07179431e-03  4.95234467e-02 -3.87903862e-02  1.81083381e-02\n",
      " -2.40933448e-02  1.10942964e-03  4.07967018e-03 -1.70246530e-02\n",
      " -2.42525451e-02  1.93311740e-03  4.51529808e-02 -3.33730318e-02\n",
      "  7.62266666e-02  1.31065995e-02  1.00874230e-02 -8.75907764e-02\n",
      " -1.88414399e-02 -2.19754800e-02 -2.40217764e-02 -5.97373322e-02\n",
      " -3.73855121e-02  1.73760112e-02  2.99498737e-02 -8.49962607e-03\n",
      "  3.64671908e-02 -4.68436582e-03 -4.77466621e-02 -1.23451941e-03\n",
      "  3.35864872e-02 -1.59965344e-02  4.58758958e-02  4.18656886e-01\n",
      "  1.21367099e-02  5.42694936e-03  1.19155459e-02  1.96602810e-02\n",
      " -8.40076618e-03  5.58388699e-03 -1.02033708e-02 -1.17206359e-02\n",
      " -3.31179351e-02  3.83885056e-02 -5.41968606e-02 -2.47945660e-04\n",
      "  1.40760494e-02 -3.80580835e-02  1.71797499e-02 -4.23982516e-02\n",
      "  8.96678343e-02 -5.05344127e-04 -9.54801310e-03 -5.47055677e-02\n",
      " -1.59261599e-02  3.70308645e-02 -2.17353459e-02 -2.07928345e-02\n",
      "  1.83075517e-02 -1.92554966e-02 -2.43045930e-02  1.04615308e-01\n",
      "  9.86108780e-02 -3.81706059e-02  5.71277402e-02  3.79569270e-02\n",
      " -1.63300056e-02  3.31352316e-02 -1.85788181e-02  2.06429455e-02\n",
      "  1.87822375e-02 -6.56616986e-02 -2.80768890e-02 -1.28687043e-02\n",
      "  3.23289714e-04  3.77898430e-03  1.80480145e-02 -4.51953989e-03\n",
      " -8.11747462e-02  1.03999786e-01 -2.70808768e-02 -4.07117046e-02\n",
      " -3.00220624e-02  3.18674631e-02  3.98861766e-02  5.17832227e-02\n",
      "  4.05683480e-02 -2.08449308e-02  4.09203395e-03  6.24044426e-02\n",
      " -2.87695024e-02 -5.02705872e-02 -4.67010848e-02 -1.47789558e-02\n",
      " -8.22763145e-02 -2.12724251e-03 -2.65510716e-02  5.37319034e-02\n",
      "  5.55519275e-02 -2.98889130e-02 -2.57312562e-02 -1.31439697e-02\n",
      "  2.11317893e-02 -4.73409854e-02  7.25218728e-02  1.58215733e-03\n",
      "  1.31616481e-02  4.04847078e-02 -2.89798547e-02  1.75786372e-02\n",
      " -6.45326674e-02 -5.25048971e-02  3.45213115e-02  6.80621639e-02\n",
      "  2.40010191e-02 -5.30634113e-02 -5.26381396e-02  2.69923694e-02\n",
      "  3.24916802e-02  2.05718353e-02 -3.33891483e-03  1.54567382e-03\n",
      "  1.00817494e-02 -1.27411196e-02 -1.22961672e-02 -1.18353060e-02\n",
      " -2.66052466e-02  3.50035578e-02  1.90403627e-03 -2.80805342e-02\n",
      " -3.63000087e-03  1.03403497e-02  5.47878770e-03 -1.17127392e-02\n",
      " -1.63864996e-02  4.46940623e-02 -1.86190885e-02 -6.82004690e-02\n",
      " -6.94678277e-02 -6.99702324e-03 -2.34558545e-02  8.27824418e-03\n",
      "  1.74027346e-02  4.00065407e-02 -3.94706242e-03 -2.77869571e-02\n",
      " -7.60728354e-03  1.22534465e-02 -2.11834572e-02  7.87648465e-03\n",
      " -3.44847851e-02  9.82257351e-03  8.00194740e-02  3.79054099e-02\n",
      " -9.84685216e-03 -4.65928540e-02 -3.12185101e-02 -3.30484152e-01\n",
      " -3.77835184e-02  2.98152808e-02 -1.94240883e-02  2.25402303e-02\n",
      " -1.03058130e-01  2.36024167e-02 -2.35691085e-03  6.01322539e-02\n",
      "  1.59889068e-02 -1.66340992e-02  2.98607815e-02 -4.46092151e-02\n",
      " -3.08812913e-02 -2.18038000e-02  3.70305553e-02 -9.42328665e-03\n",
      " -2.54218746e-02 -3.34233232e-02  1.23024441e-03 -5.64049138e-03\n",
      " -1.47319643e-03 -1.22139798e-02 -3.77102569e-02  3.61749418e-02\n",
      " -1.30693382e-02  9.63152200e-02  2.89486740e-02  5.83447441e-02\n",
      "  6.59810891e-03  4.99365851e-03  2.69358177e-02  2.90852593e-04\n",
      " -1.78785827e-02  4.19377312e-02 -2.83361366e-03  1.92323290e-02\n",
      " -6.20993376e-02  2.15495285e-02 -2.54827049e-02 -1.68942735e-02\n",
      " -1.95571184e-02 -7.19073089e-03 -4.14114371e-02 -7.01690763e-02\n",
      "  5.27856359e-03 -4.15338278e-02 -4.73003574e-02  2.83060689e-02\n",
      "  3.75323184e-02  5.35501949e-02 -3.00274696e-03  3.34542170e-02\n",
      " -1.18952226e-02 -5.15829660e-02  2.64203753e-02 -5.77647537e-02\n",
      " -1.29844155e-02 -6.93527386e-02  3.58279943e-02 -7.46885221e-03\n",
      "  1.15011269e-02 -5.51819950e-02  2.05045957e-02 -4.43250686e-03\n",
      " -1.15429778e-02  6.75643012e-02 -5.08581800e-03  1.50779840e-02\n",
      "  2.02686395e-02  7.31347408e-03  1.05381161e-01  1.09250247e-02\n",
      "  6.57389015e-02  1.59502588e-02 -2.00647376e-02 -4.91935238e-02\n",
      " -1.01679504e-01 -7.51028806e-02  4.65364987e-03  4.88458425e-02\n",
      " -1.35742570e-03  5.45472838e-02  2.17473265e-02  1.05770286e-02\n",
      " -1.55718960e-02  1.13292880e-01 -2.54805927e-04  1.69143807e-02\n",
      "  4.13836092e-02  1.81974098e-02  2.93110334e-03 -1.45094572e-02\n",
      " -2.30852198e-02  4.40638773e-02  4.74608801e-02 -2.54610568e-01\n",
      "  5.30229136e-02 -1.25374449e-02  6.66173398e-02  1.62505545e-02\n",
      "  3.19570825e-02  9.62540600e-03 -6.45515025e-02 -4.08520410e-03\n",
      "  1.27614476e-02  2.08404218e-03  6.63740113e-02  8.82849023e-02\n",
      "  4.29944098e-02 -6.04960276e-03 -6.38645235e-03  8.04580599e-02\n",
      " -6.22958913e-02  5.78164216e-03 -7.61648789e-02 -5.01784589e-03\n",
      "  1.18762199e-02  1.67964309e-01 -1.37882847e-02  3.58810984e-02\n",
      " -4.68499474e-02  1.92049965e-02 -2.86752395e-02  2.58871876e-02\n",
      "  6.01792941e-04 -2.58353595e-02  2.26860512e-02  8.19023550e-02\n",
      "  1.92107037e-02  1.96144804e-02  4.61756028e-02 -1.91134606e-02\n",
      "  3.92168500e-02 -5.71921654e-03 -1.19315565e-03  2.30876505e-02\n",
      "  4.04609255e-02 -5.29565327e-02 -3.95710170e-02  8.55334103e-02\n",
      "  6.16224576e-03 -2.10360885e-02 -3.50536741e-02 -5.15743196e-02\n",
      "  4.46386673e-02 -2.98296520e-03 -9.94408131e-03  7.41840387e-03\n",
      " -2.64240708e-02  2.37687379e-02  2.11656597e-02 -1.89520046e-02\n",
      " -2.92110257e-02 -2.00260673e-02  4.85028001e-03  1.31390635e-02\n",
      " -6.69098133e-03 -9.96955484e-03  7.69697279e-02 -5.27394079e-02]\n"
     ]
    }
   ],
   "source": [
    "# Sample embedding for a document chunk\n",
    "sample_embedding = np.array(embedding_model.embed_query(document_chunks[0].page_content))\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09730837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION: What novel techniques did the 'Attention is all you need' paper introduce?\n",
      "\n",
      "RETRIEVED CHUNKS: \n",
      "page_content='2020. URL https://arxiv.org/abs/2004.14366 .\n",
      "[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
      "Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,\n",
      "S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural\n",
      "Information Processing Systems 30 , pages 5998–6008. Curran Associates, Inc., 2017. URL\n",
      "http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .\n",
      "[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David\n",
      "Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.' metadata={'source': 'data\\\\papers\\\\rag.pdf', 'page': 14} \n",
      "\n",
      "page_content='networks , vol. 3361, no. 10, p. 1995, 1995.\n",
      "[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\n",
      "Gomez, Ł. Kaiser, and I. Polosukhin, “Attention Is All You Need,”\n",
      "Advances in neural information processing systems , vol. 30, 2017.\n",
      "[19] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\n",
      "Y . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\n",
      "with a unified text-to-text transformer,” The Journal of Machine\n",
      "Learning Research , vol. 21, no. 1, pp. 5485–5551, 2020.\n",
      "[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\n",
      "Pre-training of deep bidirectional transformers for language\n",
      "understanding,” in Proceedings of NAACL-HLT , 2019.' metadata={'source': 'data\\\\papers\\\\gen_ai_survey.pdf', 'page': 17} \n",
      "\n",
      "page_content='reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n",
      "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n",
      "described in section 3.2.\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions\n",
      "of a single sequence in order to compute a representation of the sequence. Self-attention has been\n",
      "used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n",
      "textual entailment and learning task-independent sentence representations [4, 27, 28, 22].' metadata={'source': 'data\\\\papers\\\\attention.pdf', 'page': 1} \n",
      "\n",
      "page_content='for natural language understanding. In Proceedings\n",
      "of the 2018 EMNLP Workshop BlackboxNLP: An-\n",
      "alyzing and Interpreting Neural Networks for NLP ,\n",
      "pages 353–355.\n",
      "Wei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\n",
      "granularity hierarchical attention fusion networks\n",
      "for reading comprehension and question answering.\n",
      "InProceedings of the 56th Annual Meeting of the As-\n",
      "sociation for Computational Linguistics (Volume 1:\n",
      "Long Papers) . Association for Computational Lin-\n",
      "guistics.\n",
      "Alex Warstadt, Amanpreet Singh, and Samuel R Bow-\n",
      "man. 2018. Neural network acceptability judg-\n",
      "ments. arXiv preprint arXiv:1805.12471 .\n",
      "Adina Williams, Nikita Nangia, and Samuel R Bow-' metadata={'source': 'data\\\\papers\\\\bert.pdf', 'page': 11} \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "QUESTION: List the metrics were used to compare GloVe vectors with other embedding methods such as Word2Vec?\n",
      "\n",
      "RETRIEVED CHUNKS: \n",
      "page_content='chine), a single iteration takes 14 minutes. See\n",
      "Fig. 4 for a plot of the learning curve.\n",
      "4.7 Model Analysis: Comparison with\n",
      "word2vec\n",
      "A rigorous quantitative comparison of GloVe with\n",
      "word2vec is complicated by the existence of\n",
      "many parameters that have a strong effect on per-\n",
      "formance. We control for the main sources of vari-\n",
      "ation that we identiﬁed in Sections 4.4 and 4.5 by\n",
      "setting the vector length, context window size, cor-\n",
      "pus, and vocabulary size to the conﬁguration men-\n",
      "tioned in the previous subsection.\n",
      "The most important remaining variable to con-\n",
      "trol for is training time. For GloVe, the rele-\n",
      "vant parameter is the number of training iterations.' metadata={'source': 'data\\\\papers\\\\glove.pdf', 'page': 8} \n",
      "\n",
      "page_content='The GloVe model outperforms all other methods\n",
      "on all evaluation metrics, except for the CoNLL\n",
      "test set, on which the HPCA method does slightly\n",
      "better. We conclude that the GloVe vectors are\n",
      "useful in downstream NLP tasks, as was ﬁrst\n",
      "8We use the same parameters as above, except in this case\n",
      "we found 5 negative samples to work slightly better than 10.' metadata={'source': 'data\\\\papers\\\\glove.pdf', 'page': 7} \n",
      "\n",
      "page_content='GloVe: Global Vectors for Word Representation\n",
      "Jeffrey Pennington, Richard Socher, Christopher D. Manning\n",
      "Computer Science Department, Stanford University, Stanford, CA 94305\n",
      "jpennin@stanford.edu, richard@socher.org, manning@stanford.edu\n",
      "Abstract\n",
      "Recent methods for learning vector space\n",
      "representations of words have succeeded\n",
      "in capturing ﬁne-grained semantic and\n",
      "syntactic regularities using vector arith-\n",
      "metic, but the origin of these regularities\n",
      "has remained opaque. We analyze and\n",
      "make explicit the model properties needed\n",
      "for such regularities to emerge in word\n",
      "vectors. The result is a new global log-\n",
      "bilinear regression model that combines\n",
      "the advantages of the two major model' metadata={'source': 'data\\\\papers\\\\glove.pdf', 'page': 0} \n",
      "\n",
      "page_content='type of weighting scheme proposed in our model.\n",
      "Table 3 shows results on ﬁve different word\n",
      "similarity datasets. A similarity score is obtained\n",
      "from the word vectors by ﬁrst normalizing each\n",
      "feature across the vocabulary and then calculat-\n",
      "ing the cosine similarity. We compute Spearman’s\n",
      "rank correlation coefﬁcient between this score and\n",
      "the human judgments. CBOW∗denotes the vec-\n",
      "tors available on the word2vec website that are\n",
      "trained with word and phrase vectors on 100B\n",
      "words of news data. GloVe outperforms it while\n",
      "using a corpus less than half the size.\n",
      "Table 4 shows results on the NER task with the\n",
      "CRF-based model. The L-BFGS training termi-' metadata={'source': 'data\\\\papers\\\\glove.pdf', 'page': 7} \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstration of the retriever finding the relevant document chunks\n",
    "questions = [\n",
    "    \"What novel techniques did the 'Attention is all you need' paper introduce?\",\n",
    "    \"List the metrics were used to compare GloVe vectors with other embedding methods such as Word2Vec?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f'QUESTION: {question}' + '\\n')\n",
    "\n",
    "    retrieved_chunks = retrieval_system.invoke(question)\n",
    "    print('RETRIEVED CHUNKS: ')\n",
    "    for chunk in retrieved_chunks:\n",
    "        print(chunk, '\\n')\n",
    "\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76fd50",
   "metadata": {},
   "source": [
    "## 1.2) Implementing the RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbcf42",
   "metadata": {},
   "source": [
    "Building applications with Large Language Models (LLM's) requires permissions from the LLM provider via an API key. There are two types of LLM models available for use:\n",
    "<ol>\n",
    "    <li>Open-source models, which are usually smaller models with lesser capabilities (e.g. Llama created by FaceBook, Flan-T5 created by Google)</li>\n",
    "    <li>Proprietary models, which are usually larger with better performace (e.g. GPT-4o by OpenAI, Gemini-1.5 by Google etc.), but aren't free to use.</li>\n",
    "</ol>\n",
    "\n",
    "A free-to-use, open-source HuggingFace LLM will be used here. In order to use a HuggingFace LLM, there are some steps that first need to completed:\n",
    "<ul>\n",
    "    <li>Create a <a href= \"https://huggingface.co/\">Huggingface</a> Account</li>\n",
    "    <li>Create a new <a href='https://huggingface.co/settings/tokens'>API Access Token</a> with the <strong>WRITE</strong> Token Type. <strong>Save this access token in a secure place and do not share it with others.</strong> <strong>Save the token to a <a href=\"https://dev.to/jakewitcher/using-env-files-for-environment-variables-in-python-applications-55a1\">.env file</a>.</strong> </li>\n",
    "    <li>Accept the terms and conditions. This step may be required for certain models like <a href= 'https://huggingface.co/mistralai/Mistral-7B-v0.1'>mistralai/Mistral-7B-v0.1</a>. View the status of the model request <a href='https://huggingface.co/settings/gated-repos'>here</a></li>\n",
    "</ul>\n",
    "\n",
    "The model must also be hosted on HuggingFace Spaces. To do this:\n",
    "<ul>\n",
    "  <li>Go to: <a href= https://huggingface.co/spaces>Spaces</a></li>\n",
    "  <li>Click \"+ New Space\" in the top right</li>\n",
    "  <li>Fill in:\n",
    "    <ul>\n",
    "      <li>Space name: Choose any</li>\n",
    "      <li>SDK: Choose Gradio (blank template)</li>\n",
    "      <li>Space hardware: Choose CPU basic (free)</li>\n",
    "      <li>Visibility: Private</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Click: Create Space</li>\n",
    "  <li>Select Files in the top right which will direct to a new page</li>\n",
    "  <li>Select + Contribute in the top right</li>\n",
    "  <li>Select Upload files</li>\n",
    "  <li>Add the files in hf_spaces folder provided to user space, modifying them appropriately for the models</li>\n",
    "</ul>\n",
    "\n",
    "The .env file will need the following:\n",
    "* Set `HUGGINGFACE_API_KEY`: Obtain the Huggingface API key\n",
    "* Set `GRADIO_SPACE_NAME`: Set the Gradio space name after creating it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0050b4",
   "metadata": {},
   "source": [
    "### 1.2.1) Choosing an LLM and Initalizing the Retriever System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed3d9c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat .env\n",
    "# !rm .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eb4d848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://ojw92-cs8803-hw4.hf.space ✔\n",
      "\u001b[1mGradioLLMWrapper\u001b[0m\n",
      "Params: {}\n",
      "Your retriever is of type VectorStoreRetriever: True\n",
      "Your retriever returns chunks from the expected relevant documents: True\n"
     ]
    }
   ],
   "source": [
    "from rag_chain import RAG_Chain\n",
    "from local_tests.retriever_test import Retriever_Test\n",
    "from langchain_core.vectorstores.base import VectorStoreRetriever\n",
    "\n",
    "# Instantiate + configure the RAG_Chain class to use a HF-hosted Gradio Space via a custom LLM wrapper\n",
    "rag_chain = RAG_Chain(data_dir='./data/papers/', llm_type=\"gradio_flan\")\n",
    "local_test = Retriever_Test()\n",
    "\n",
    "# Print the LLM used\n",
    "print(rag_chain.llm)\n",
    "\n",
    "# Local test for RAG retriever system\n",
    "\n",
    "# Check that retriever is of expected type:\n",
    "rag_retriever = rag_chain.retriever_system\n",
    "type_test = isinstance(rag_retriever, VectorStoreRetriever)\n",
    "print('Your retriever is of type VectorStoreRetriever:', type_test)\n",
    "\n",
    "# Check that the retriever retrieves relevant chunks\n",
    "output_retrieved_chunks = rag_retriever.invoke(local_test.relevance_prompt)\n",
    "relevance_test = True\n",
    "for chunk in output_retrieved_chunks:\n",
    "    if local_test.relevant_pdf not in chunk.metadata['source']:\n",
    "        relevance_test = False\n",
    "print('Your retriever returns chunks from the expected relevant documents:', relevance_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd18732",
   "metadata": {},
   "source": [
    "### 1.2.2) Formatting the Prompt with PromptTemplates\n",
    "\n",
    "Prompts are a set of instructions that are given to an LLM in order to guide it to produce responses that are coherent, contextual and relevant. It usually takes several edits and changes to the prompt until the LLM produces the desirable response. This process is called Prompt Engineering. It is common practice to save a good, desirable prompt as a template for any time this prompt is needed to be used again. This is done with PromptTemplates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e8b31",
   "metadata": {},
   "source": [
    "The **createPrompt** function in **rag_chain.py** takes as an input parameter, a dictionary that stores a question along with 4 possible answer choices. For example, the input parameter could look like:\n",
    "\n",
    "{ <br>\n",
    "   &emsp; 'question': \"What is the main contribution of the Transformer architecture?\", <br>\n",
    "   &emsp; 'A': \"It introduces convolutional layers for sequence tasks.\", <br>\n",
    "   &emsp; 'B': \"It improves word embeddings using context.\", <br>\n",
    "   &emsp; 'C': \"It removes recurrence and uses self-attention mechanisms.\", <br>\n",
    "   &emsp; 'D': \"It uses RNNs for language modeling.\" <br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "862b7c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Question: \n",
      "A: \n",
      "B: \n",
      "C: \n",
      "D: \n",
      "Please select the best answer and explain your choice.\n"
     ]
    }
   ],
   "source": [
    "# Print the empty prompt template\n",
    "prompt_template = rag_chain.createPrompt(question={'question': \"\", \"A\": \"\", \"B\": \"\", \"C\": \"\", \"D\": \"\"})\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "424710bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Question: What is the main contribution of the Transformer architecture?\n",
      "A: It introduces convolutional layers for sequence tasks.\n",
      "B: It improves word embeddings using context.\n",
      "C: It removes recurrence and uses self-attention mechanisms.\n",
      "D: It uses RNNs for language modeling.\n",
      "Please select the best answer and explain your choice.\n"
     ]
    }
   ],
   "source": [
    "# Print the formatted prompt with the question. This is what will be passed to the LLM.\n",
    "from local_tests.rag_test import RAG_Test\n",
    "tests = RAG_Test()\n",
    "\n",
    "prompt_with_question = rag_chain.createPrompt(question=tests.question1)\n",
    "print(prompt_with_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02500eae",
   "metadata": {},
   "source": [
    "### 1.2.3) Creating Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cbe028",
   "metadata": {},
   "source": [
    "In LangChain, \"chains\" are a core concept designed to manage and streamline interactions with language models. They allow creating sequences of operations where the output of one step can be used as the input for the next. This is particularly useful for building complex workflows and applications that involve multiple stages of processing.\n",
    "\n",
    "Langchain offers a RetrievalQA chain, which combines the retriever module with a QA chain (short for Question-Answering). The retriever is used to retrieve relevant documents from the vectorstore, and the QA chain answers questions based on the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e1ccac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = rag_chain.createRAGChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11e32d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Tests for end-to-end RAG pipeline \n",
      "\n",
      "\n",
      "Human: Question: What is the main contribution of the Transformer architecture?\n",
      "A: It introduces convolutional layers for sequence tasks.\n",
      "B: It improves word embeddings using context.\n",
      "C: It removes recurrence and uses self-attention mechanisms.\n",
      "D: It uses RNNs for language modeling.\n",
      "Please select the best answer and explain your choice.\n",
      "Answer selected by the LLM: C\n",
      "Correct answer: C\n",
      "Human: Question: The Transformer architecture relies heavily on which mechanism to process information?\n",
      "A: Convolution\n",
      "B: Self-attention\n",
      "C: Pooling\n",
      "D: Dropout\n",
      "Please select the best answer and explain your choice.\n",
      "Answer selected by the LLM: B\n",
      "Correct answer: B\n",
      "Human: Question: What is the primary innovation introduced by BERT?\n",
      "A: Unidirectional language models\n",
      "B: Bidirectional training of transformers\n",
      "C: Convolutional embeddings\n",
      "D: Memory networks\n",
      "Please select the best answer and explain your choice.\n",
      "Answer selected by the LLM: B\n",
      "Correct answer: B\n",
      "Human: Question: BERT is pre-trained on which type of task?\n",
      "A: Named entity recognition\n",
      "B: Image classification\n",
      "C: Machine translation\n",
      "D: Next sentence prediction and masked language modeling\n",
      "Please select the best answer and explain your choice.\n",
      "Answer selected by the LLM: D\n",
      "Correct answer: D\n",
      "Human: Question: What is the key innovation introduced in the paper 'Attention is All You Need'?\n",
      "A: The Transformer architecture\n",
      "B: Recurrent Neural Networks\n",
      "C: Long Short-Term Memory (LSTM)\n",
      "D: Bahdanau attention\n",
      "Please select the best answer and explain your choice.\n",
      "Answer selected by the LLM: A\n",
      "Correct answer: A\n",
      "Human: Question: GloVe primarily uses which mathematical construct to represent word vectors?\n",
      "A: Recurrent networks\n",
      "B: Singular value decomposition\n",
      "C: Word co-occurrence matrix\n",
      "D: Self-attention mechanism\n",
      "Please select the best answer and explain your choice.\n",
      "Answer selected by the LLM: C\n",
      "Correct answer: C\n",
      "Human: Question: What is the key contribution of the paper 'Improving Language Understanding by Generative Pre-Training (GPT)'?\n",
      "A: Introduction of attention mechanisms\n",
      "B: Building deep convolutional networks\n",
      "C: Combining LSTM and CNN architectures\n",
      "D: Pre-training on large datasets followed by fine-tuning for specific tasks\n",
      "Please select the best answer and explain your choice.\n",
      "Answer selected by the LLM: D\n",
      "Correct answer: D\n",
      "Human: Question: What is the key concept introduced in the RAG framework?\n",
      "A: Combining retrieval-based methods with generative models\n",
      "B: Unsupervised pre-training on large corpora\n",
      "C: End-to-end training of neural networks\n",
      "D: Using reinforcement learning for text generation\n",
      "Please select the best answer and explain your choice.\n",
      "Answer selected by the LLM: D\n",
      "Correct answer: A\n",
      "7/8 questions answered correctly\n"
     ]
    }
   ],
   "source": [
    "from local_tests.rag_test import RAG_Test\n",
    "\n",
    "tests = RAG_Test()\n",
    "\n",
    "print('Local Tests for end-to-end RAG pipeline', '\\n\\n')\n",
    "\n",
    "questions = tests.local_test_questions\n",
    "answers = tests.local_test_answers\n",
    "\n",
    "correct = 0\n",
    "total = len(questions)\n",
    "for i in range(len(questions)):\n",
    "    # Create the prompt with a question\n",
    "    prompt_with_question = rag_chain.createPrompt(question=questions[i])\n",
    "    print(prompt_with_question)\n",
    "\n",
    "    # Query the LLM\n",
    "    response = qa_chain(prompt_with_question)\n",
    "\n",
    "    print('Answer selected by the LLM:', response['result'])\n",
    "    print('Correct answer:', answers[i])\n",
    "\n",
    "    if response['result'] == answers[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(f'{correct}/{total} questions answered correctly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6873ad",
   "metadata": {},
   "source": [
    "## 2) Hosting and Deploying LLM and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e77859",
   "metadata": {},
   "source": [
    "This section will explore Ollama-hosted and Flask Ollama-hosted LLMs within the RAG pipeline created in the previous question. The RAG system will be deployed using a Flask container, simulating a real-world LLM Pipeline deployment.\n",
    "\n",
    "This section will compare different hosting methods:\n",
    "\n",
    "- **Cloud-based LLMs** (Hugging Face Hub or Spaces) offload the burden of compute but require external API calls.\n",
    "- **Local LLMs** (Ollama) offer greater control over data, cost, and customizations, but compute is limited by hardware.\n",
    "- **Network-hosted LLMs** (Flask Ollama) allow for remote access within a private infrastructure, which is useful for on-premise deployments in industries like healthcare and finance where data privacy is critical.\n",
    "  \n",
    "In many real-world applications, LLMs are accessed through APIs rather than used directly. In section 1), Hugging Face’s API was used via Hugging Face Spaces. Now, the same RAG pipeline will be deployed but using Ollama and Flask.\n",
    "\n",
    "The approach presented here is only one way to accomplish the task, but serves as an introduction and a vital opportunity to experiment with different deployment strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa398706",
   "metadata": {},
   "source": [
    "## 2.1) Using Ollama with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35563690",
   "metadata": {},
   "source": [
    "This section introduces Ollama and use the Ollama LLM in the RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce07f814",
   "metadata": {},
   "source": [
    "### 2.1.1) Getting Started with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5db71",
   "metadata": {},
   "source": [
    "First, [Ollama](https://ollama.com/) needs to be downloaded and the following model will need to be pulled:\n",
    "- `llama3.2`\n",
    "  \n",
    "Refer to [ollama-python Documentation](https://github.com/ollama/ollama-python) (prerequisites section) and [Quickstart Guide](https://github.com/ollama/ollama/blob/main/README.md#quickstart) as necessary. Make sure the ollama server is running on the machine prior to running the local test below.\n",
    "\n",
    "**Ollama must be kept running for the next sections to work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052488e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@abonia/running-ollama-in-google-colab-free-tier-545609258453\n",
    "# !ollama run gemma3\n",
    "!pip install colab-xterm\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm\n",
    "\n",
    "# curl https://ollama.ai/install.sh | sh\n",
    "# ollama serve &        # start the server\n",
    "# ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca88cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=(true | false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafff2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline as tap\n",
    "from local_tests.deploy_test import Deploy_Test\n",
    "\n",
    "local_test = Deploy_Test()\n",
    "\n",
    "# Check that Ollama returns the expected response\n",
    "response = tap.query_ollama(local_test.ollama_query)\n",
    "response_check = response == local_test.ollama_response\n",
    "print('Your Ollama server returned the expected response:', response_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8db7338",
   "metadata": {},
   "source": [
    "### 2.1.2) Using Ollama with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_tests.deploy_test import Deploy_Test\n",
    "from rag_chain import RAG_Chain\n",
    "\n",
    "# Local test for RAG using Ollama LLM\n",
    "\n",
    "# Check that Ollama works with rag_chain.py\n",
    "rag_chain_oo = RAG_Chain(data_dir='./data/papers/', llm_type=\"ollama_only\", init_retriever=False)\n",
    "rag_chain_oo.llm.temperature = 0\n",
    "local_test = Deploy_Test()\n",
    "\n",
    "# Printing the LLM used\n",
    "print(f\"LLM Info:\\n {rag_chain_oo.llm}\\n\")\n",
    "\n",
    "# Check that RAG returns the expected response\n",
    "response = rag_chain_oo.query_the_llm(local_test.ollama_rag_query)\n",
    "response_check = response == local_test.ollama_rag_response\n",
    "print('Your Ollama LLM in the RAG system returned the expected response:', response_check)\n",
    "print(\"\\nQUERY:\", local_test.ollama_rag_query)\n",
    "print(\"EXPECTED RESPONSE:\", local_test.ollama_rag_response)\n",
    "print(f\"Your Response: {response}\" if not response_check else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fd6b0",
   "metadata": {},
   "source": [
    "## 2.2) Deploying an LLM and using it with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e498c0",
   "metadata": {},
   "source": [
    "In the next steps, the Ollama LLM from 2.1.1) will be deployed by containerizing the LLM using Flask. While Ollama works locally, Flask can expose the LLM for network use. It is possible to query into this Flask Ollama LLM by reusing Langchain's OpenAI wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36fb05",
   "metadata": {},
   "source": [
    "### 2.2.1) Using Flask to Containerize the Ollama LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d57ff",
   "metadata": {},
   "source": [
    "In the terminal or command prompt, run the llm_app.py file before running the local test cell below.\n",
    "\n",
    "**Keep llm_app.py running to complete the next section - do not terminate it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4fbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the llm_app.py file running in the background for the next test cell\n",
    "# !python llm_app.py &\n",
    "import subprocess\n",
    "\n",
    "process = subprocess.Popen(['python', 'llm_app.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4370e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline as tap\n",
    "from local_tests.deploy_test import Deploy_Test\n",
    "\n",
    "local_test = Deploy_Test()\n",
    "\n",
    "# Local test for Flask Ollama server\n",
    "\n",
    "#Check that Ollama returns the expected response\n",
    "response = tap.query_flask_ollama(local_test.ollama_query)\n",
    "response_check = response == local_test.ollama_response\n",
    "print('Your Flask Ollama server returned the expected response:', response_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa737d",
   "metadata": {},
   "source": [
    "### 2.2.2) Using Flask Ollama with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2df871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline as tap\n",
    "from local_tests.deploy_test import Deploy_Test\n",
    "from rag_chain import RAG_Chain\n",
    "\n",
    "local_test = Deploy_Test()\n",
    "\n",
    "# Local test for RAG using Flask Ollama\n",
    "\n",
    "# Check that Ollama works with rag_chain.py\n",
    "rag_chain_fo = RAG_Chain(data_dir='./data/papers/', llm_type=\"flask_ollama\", init_retriever=False)\n",
    "rag_chain_fo.llm.temperature = 0\n",
    "local_test = Deploy_Test()\n",
    "\n",
    "# Printing the LLM used\n",
    "print(f\"LLM Info:\\n {rag_chain_fo.llm}\\n\")\n",
    "\n",
    "# Check that RAG returns the expected response\n",
    "response = rag_chain_fo.query_the_llm(local_test.flask_ollama_rag_query)\n",
    "response_check = response == local_test.flask_ollama_rag_response\n",
    "print('Your Flask Ollama LLM in the RAG system returned the expected response:', response_check)\n",
    "print(\"\\nQUERY:\", local_test.flask_ollama_rag_query)\n",
    "print(\"EXPECTED RESPONSE:\", local_test.flask_ollama_rag_response)\n",
    "print(f\"YOUR RESPONSE: {response}\" if not response_check else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57932833",
   "metadata": {},
   "source": [
    "## 2.3) Deploying RAG using Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced07cab",
   "metadata": {},
   "source": [
    "### 2.3.1) Deploying RAG Local Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11750bd",
   "metadata": {},
   "source": [
    "In the terminal or command prompt, run the rag_app.py file before running the local test cell below.\n",
    "\n",
    "**Keep rag_app.py running to complete the next section - do not terminate it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23be41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_llm = subprocess.Popen(['python', 'llm_app.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the rag_app.py file running in the background for the next test cell\n",
    "# !python rag_app.py &\n",
    "import subprocess\n",
    "\n",
    "process_rag = subprocess.Popen(['python', 'rag_app.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3b455",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python rag_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d33541",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RESPONSE:\", response)\n",
    "print('================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17bdfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def is_flask_up(port):\n",
    "    try:\n",
    "        r = requests.get(f\"http://127.0.0.1:{port}/\")\n",
    "        # Can check r.status_code or r.json()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "with open(\"rag_port.txt\") as f:\n",
    "    port = int(f.read().strip())\n",
    "\n",
    "if is_flask_up(port):\n",
    "    print(f\"Flask server is running on port {port}!\")\n",
    "else:\n",
    "    print(f\"Flask server is NOT running on port {port}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453457a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline as tap\n",
    "from local_tests.deploy_test import Deploy_Test\n",
    "\n",
    "local_test = Deploy_Test()\n",
    "\n",
    "# Local test for Ollama server\n",
    "\n",
    "# Check that flask rag returns the expected response\n",
    "response = tap.query_flask_rag(local_test.flask_rag_query)\n",
    "response_check = response[0][\"text\"] == local_test.flask_rag_response\n",
    "print('Your Flask RAG system returned the expected response:', response_check)\n",
    "print(\"\\nQUERY:\", local_test.flask_rag_query)\n",
    "print(\"EXPECTED RESPONSE:\", local_test.flask_rag_response)\n",
    "print(f\"YOUR RESPONSE: {response[0]['text']}\" if not response_check else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc29024",
   "metadata": {},
   "source": [
    "### 2.3.2) Deploying RAG Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pipeline as tap\n",
    "\n",
    "tap.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423c1a8",
   "metadata": {},
   "source": [
    "**Running the pipeline should produce hashed_answers.json**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_flask_ollama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
