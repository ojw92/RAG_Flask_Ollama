{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c59227f",
   "metadata": {},
   "source": [
    "# RAG_Flask_Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338667b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import langchain\n",
    "import langchain_community\n",
    "import langchain_core\n",
    "import langchain_openai\n",
    "import sentence_transformers\n",
    "import pypdf\n",
    "import dotenv\n",
    "import importlib.metadata\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "print('Version information')\n",
    "\n",
    "print('python: {}'.format(sys.version))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('langchain: {}'.format(langchain.__version__))\n",
    "print('langchain_community: {}'.format(langchain_community.__version__))\n",
    "print('sentence_transformers: {}'.format(sentence_transformers.__version__))\n",
    "print('pypdf: {}'.format(pypdf.__version__))\n",
    "print('langchain_core: {}'.format(langchain_core.__version__))\n",
    "print('langchain_openai: {}'.format(importlib.metadata.version(\"langchain-openai\")))\n",
    "print('python-dotenv: {}'.format(importlib.metadata.version(\"python-dotenv\")))\n",
    "print('ollama: {}'.format(importlib.metadata.version(\"ollama\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787aa45",
   "metadata": {},
   "source": [
    "## 1) Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faef326",
   "metadata": {},
   "source": [
    "State-of-the-art Large Language Models (LLM's) are trained on an enormous corpus of textual data (taken from webpages, articles, books etc.), and they store a wide range of general knowledge in their parameters. While they are able to perform well on tasks that require general knowledge, they tend to struggle on tasks that require information that wasn't present in the training data. For instance, LLM's may struggle on tasks that require knowledge of domain-specific information, or even up-to-date information.\n",
    "\n",
    "It is very important to overcome this problem, because it is undesirable to get a non-answer from the LLM, and potentially even dangerous if the LLM begins to hallucinate (i.e ramble on with an answer that seems believable but is factually inaccurate). Therefore, it is essential to bridge the gap between the LLM's general knowledge, and other domain-specific or up-to-date information in order to help the LLM generate responses that are contextual and factually accurate, while reducing the chances of hallucinations.\n",
    "\n",
    "There are two effective ways of accomplishing this:\n",
    "1. <strong>Fine-tuning the LLM on the domain-specific/proprietary/new data</strong>:\n",
    "    <ul>\n",
    "        <li>By fine-tuning the model, it can be made suitable for the task.</li>\n",
    "        <li>However, this comes with some limitations. It is compute-intensive, expensive and not agile (it's not realistic to fine-tune an LLM with the new data coming in everyday)</li>\n",
    "    </ul>\n",
    "2. <strong>Retrieval Augmented Generation (RAG)</strong>:\n",
    "    <ul>\n",
    "        <li><a href='https://arxiv.org/abs/2005.11401'>This technique</a> provides the LLM with contextual information from an external knowledge source that can be updated more easily.</li>\n",
    "        <li>It allows the LLM to generate more contextual, and factually accurate responses by allowing it to dynamically access information from an external knowledge source.</li>\n",
    "    </ul>\n",
    "\n",
    "<center><img src=\"data/images/rag-architecture.png\" alt=\"drawing\" width=\"700\" align='center'></center>\n",
    "<center>Basic RAG Architecture</center>\n",
    "\n",
    "\n",
    "The RAG pipeline consists of the following steps:\n",
    "<ol>\n",
    "    <li><strong>Retrieval</strong>: The user's query is used to retrieve the relevant contextual information from the external knowledge source. The external knowledge source is a vector store that contains the embeddings of the documents that contain the proprietary/domain-specific data. The user query is embedded into the same vector space as these documents, and a similarity search is performed in this embedding space to retrieve the documents that are most similar to the user's query. These retrieved documents make up the context that the LLM will consider in order to produce factually accurate responses. </li>\n",
    "    <li><strong>Augmentation</strong>: The user's query is augmented with the retrieved contextual information to form the prompt. The prompt will also usually include instructions to the LLM for performing the task. There is an entire sub-field known as Prompt Engineering, that is dedicated to fine-tuning the prompt so as to get the best possible response from the LLM, and you will get some experience with it in <strong>7.2</strong> while implementing the RAG Chain. </li>\n",
    "    <li><strong>Generation</strong>: The constructed prompt, including the instructions to the LLM, the user query and the retrieved context, is fed to the LLM to generate a response that is comprehensible and factually correct.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b601b03f",
   "metadata": {},
   "source": [
    "## 1.1) Implementing the Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d547bba",
   "metadata": {},
   "source": [
    "In the first step, the Retriever for the RAG Chain will be implemented. The documents serving as the basis of the external knowledge source will be split into smaller chunks to be used with the vector database. Lastly, the Retriever object will be created to be used with the RAG Chain to retrieve relevant document chunks as additional contextual information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92345806",
   "metadata": {},
   "source": [
    "### 1.1.1) Loading and Pre-processing data: Document Loaders and Text Splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af00cb82",
   "metadata": {},
   "source": [
    "Langchain provides <a href='https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/'>several classes</a> to load data into Document objects. There are classes to load data from HTML files, PDF files, JSON files, CSV files, file directories etc. Here, Langchain's <a href='https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/#pypdf-directory'>PyPDFDirectoryLoader</a> will be used to load PDF's from a directory. A Document object is a dictionary that stores the text and metadata about each document.\n",
    "\n",
    "Once the data has been loaded into Document(s), it's common to split the documents into smaller chunks. This is done because when a user inputs a query to the system, the retriever will return the most relevant documents, which will be augmented to the prompt that is sent to the LLM. There are limits to the length of this prompt, since it must fit into the LLM's context window. Therefore, it's common to split documents into smaller chunks, so that only the retrieved relevant chunks are inserted into the prompt. Langchain offers several <a href='https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/'>TextSplitters</a> to perform the document splitting. RecursiveCharacterTextSplitter, which is a way to split a document into several chunks in a manner such that related pieces of text are kept together in a chunk, will be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4fb04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retriever import Retriever\n",
    "from local_tests.retriever_test import Retriever_Test\n",
    "\n",
    "local_test = Retriever_Test()\n",
    "stu_retriever = Retriever()\n",
    "num_chunks_to_query = 2\n",
    "\n",
    "print('Local Tests for Loading and Splitting Documents \\n')\n",
    "\n",
    "# Local test for load documents\n",
    "output_documents = stu_retriever.loadDocuments(data_dir='./data/papers/')\n",
    "load_test = (len(output_documents) == local_test.load_documents_len)\n",
    "print('Your load documents works as expected:', load_test)\n",
    "\n",
    "# Local test for split documents\n",
    "output_chunks = stu_retriever.splitDocuments(output_documents, chunk_size=700, chunk_overlap=50)\n",
    "split_test = (len(output_chunks) == local_test.split_documents_len)\n",
    "print('Your split documents works as expected:', split_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ab343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF documents\n",
    "from retriever import Retriever\n",
    "retriever = Retriever()\n",
    "documents = retriever.loadDocuments(data_dir='./data/papers/')\n",
    "\n",
    "print('documents type:', type(documents))           # python list\n",
    "print('documents[0] type:', type(documents[0]))     # Document object\n",
    "print('documents length:', len(documents))          # each page is loaded as a separate document (104 pages -> 104 documents)\n",
    "\n",
    "print('\\nContent of documents[0]:\\n', documents[0]) # the first document object, containing 'page_content' and 'metadata' fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59310dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into smaller chunks\n",
    "document_chunks = retriever.splitDocuments(documents)\n",
    "\n",
    "print('document_chunks type:', type(document_chunks)) # python list\n",
    "print('document_chunks[0] type:', type(document_chunks[0])) # Document object\n",
    "print('document_chunks length:', len(document_chunks)) # you should observe that each document (corresponding to a page in a PDF) has been split into several chunks\n",
    "\n",
    "for chunk in document_chunks[:3]:   # displaying the first 3 chunks\n",
    "    print('\\n\\n', chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dde4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of chunk size\n",
    "def compute_avg_chunk_size(chunks):\n",
    "    return sum([len(chunk.page_content) for chunk in chunks])/len(chunks)\n",
    "\n",
    "print(f'Before split, there were {len(documents)} documents, with average size equal to {compute_avg_chunk_size(documents)}.')\n",
    "print(f'After split, there were {len(document_chunks)} documents (chunks), with average size equal to {compute_avg_chunk_size(document_chunks)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25cb2d",
   "metadata": {},
   "source": [
    "### 1.1.2) Creating the Vector Database and Retrieval System: Embedding models and Vector Stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc6911b",
   "metadata": {},
   "source": [
    "<a href='https://python.langchain.com/v0.2/docs/integrations/text_embedding/'>Langchain provides several embedding models</a>. An embedding model converts a piece of natural language (e.g. a token) into a vector embedding. The next steps will involve using Huggingface's BGE Embedding models, which are the one of the best open-source embedding models <a href='https://python.langchain.com/v0.2/docs/integrations/text_embedding/bge_huggingface/'>(according to Langchain)</a>. The model used (BAAI/bge-small-en-v1.5) has 384-dimensional embedding vectors.\n",
    "\n",
    "After obtaining the embedding model, vectorstore needs to be created by embedding all of the document chunks into the vector space. A retriever will be used to retrieve the document chunks from the vectorstore that are most similar to the user's query using efficient similarity search algorithms. Langchain offers <a href='https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/'>several vectorstores</a>, and Facebook's AI Similarity Search (FAISS) will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retriever import Retriever\n",
    "from local_tests.retriever_test import Retriever_Test\n",
    "\n",
    "local_test = Retriever_Test()\n",
    "stu_retriever = Retriever()\n",
    "num_chunks_to_query = 2\n",
    "\n",
    "print('Local Tests for Retriever \\n')   # ensure that the local tests for the loading and splitting documents pass\n",
    "\n",
    "output_documents = stu_retriever.loadDocuments(data_dir='./data/papers/')\n",
    "output_chunks = stu_retriever.splitDocuments(output_documents, chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Local test for retriever embeddings\n",
    "output_retrieval_system = stu_retriever.createRetriever(output_chunks, num_chunks_to_return=num_chunks_to_query)\n",
    "student_embedding_model = stu_retriever.huggingface_embeddings\n",
    "output_embedding = np.array(student_embedding_model.embed_query(output_chunks[0].page_content))\n",
    "embedding_shape = output_embedding.shape[0]\n",
    "\n",
    "if (student_embedding_model.model_name == local_test.model_name):\n",
    "    embedding_shape_test = (embedding_shape == local_test.embedding_size)\n",
    "    print('Your retriever embeddings returns the expected shape:', embedding_shape_test)\n",
    "else:\n",
    "    print('You are free to choose the embedding model to use for the best results (provided it works with Gradescope),')\n",
    "    print(f'but we can only locally test the embedding shape (no value testing) if using the {local_test.model_name} model')\n",
    "    print('and the RecursiveCharacterTextSplitter.')\n",
    "\n",
    "# Local test for chunk relevance\n",
    "output_retrieved_chunks = output_retrieval_system.invoke(local_test.relevance_prompt)\n",
    "returned_count_test = (len(output_retrieved_chunks) == num_chunks_to_query)\n",
    "relevance_test = True\n",
    "for chunk in output_retrieved_chunks:\n",
    "    if local_test.relevant_pdf not in chunk.metadata['source']:\n",
    "        relevance_test = False\n",
    "print('Your retriever returns the expected number of chunks:', returned_count_test)\n",
    "print('Your retriever returns chunks from the expected relevant documents:', relevance_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d47f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the retriever\n",
    "retrieval_system = retriever.createRetriever(document_chunks)\n",
    "embedding_model = retriever.huggingface_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af441221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample embedding for a document chunk\n",
    "sample_embedding = np.array(embedding_model.embed_query(document_chunks[0].page_content))\n",
    "print(\"Size of the embedding: \", sample_embedding.shape)\n",
    "print(\"Sample embedding of a document chunk: \", sample_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09730837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of the retriever finding the relevant document chunks\n",
    "questions = [\n",
    "    \"What novel techniques did the 'Attention is all you need' paper introduce?\",\n",
    "    \"List the metrics were used to compare GloVe vectors with other embedding methods such as Word2Vec?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f'QUESTION: {question}' + '\\n')\n",
    "\n",
    "    retrieved_chunks = retrieval_system.invoke(question)\n",
    "    print('RETRIEVED CHUNKS: ')\n",
    "    for chunk in retrieved_chunks:\n",
    "        print(chunk, '\\n')\n",
    "\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76fd50",
   "metadata": {},
   "source": [
    "## 1.2) Implementing the RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbcf42",
   "metadata": {},
   "source": [
    "Building applications with Large Language Models (LLM's) requires permissions from the LLM provider via an API key. There are two types of LLM models available for use:\n",
    "<ol>\n",
    "    <li>Open-source models, which are usually smaller models with lesser capabilities (e.g. Llama created by FaceBook, Flan-T5 created by Google)</li>\n",
    "    <li>Proprietary models, which are usually larger with better performace (e.g. GPT-4o by OpenAI, Gemini-1.5 by Google etc.), but aren't free to use.</li>\n",
    "</ol>\n",
    "\n",
    "A free-to-use, open-source HuggingFace LLM will be used here. In order to use a HuggingFace LLM, there are some steps that first need to completed:\n",
    "<ul>\n",
    "    <li>Create a <a href= \"https://huggingface.co/\">Huggingface</a> Account</li>\n",
    "    <li>Create a new <a href='https://huggingface.co/settings/tokens'>API Access Token</a> with the <strong>WRITE</strong> Token Type. <strong>Save this access token in a secure place and do not share it with others.</strong> <strong>Save the token to a <a href=\"https://dev.to/jakewitcher/using-env-files-for-environment-variables-in-python-applications-55a1\">.env file</a>.</strong> </li>\n",
    "    <li>Accept the terms and conditions. This step may be required for certain models like <a href= 'https://huggingface.co/mistralai/Mistral-7B-v0.1'>mistralai/Mistral-7B-v0.1</a>. View the status of the model request <a href='https://huggingface.co/settings/gated-repos'>here</a></li>\n",
    "</ul>\n",
    "\n",
    "The model must also be hosted on HuggingFace Spaces. To do this:\n",
    "<ul>\n",
    "  <li>Go to: <a href= https://huggingface.co/spaces>Spaces</a></li>\n",
    "  <li>Click \"+ New Space\" in the top right</li>\n",
    "  <li>Fill in:\n",
    "    <ul>\n",
    "      <li>Space name: Choose any</li>\n",
    "      <li>SDK: Choose Gradio (blank template)</li>\n",
    "      <li>Space hardware: Choose CPU basic (free)</li>\n",
    "      <li>Visibility: Private</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li>Click: Create Space</li>\n",
    "  <li>Select Files in the top right which will direct to a new page</li>\n",
    "  <li>Select + Contribute in the top right</li>\n",
    "  <li>Select Upload files</li>\n",
    "  <li>Add the files in hf_spaces folder provided to user space, modifying them appropriately for the models</li>\n",
    "</ul>\n",
    "\n",
    "The .env file will need the following:\n",
    "* Set `HUGGINGFACE_API_KEY`: Obtain the Huggingface API key\n",
    "* Set `GRADIO_SPACE_NAME`: Set the Gradio space name after creating it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0050b4",
   "metadata": {},
   "source": [
    "### 1.2.1) Choosing an LLM and Initalizing the Retriever System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d9c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat .env\n",
    "# !rm .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_chain import RAG_Chain\n",
    "from local_tests.retriever_test import Retriever_Test\n",
    "from langchain_core.vectorstores.base import VectorStoreRetriever\n",
    "\n",
    "# Instantiate + configure the RAG_Chain class to use a HF-hosted Gradio Space via a custom LLM wrapper\n",
    "rag_chain = RAG_Chain(data_dir='./data/papers/', llm_type=\"gradio_flan\")\n",
    "local_test = Retriever_Test()\n",
    "\n",
    "# Print the LLM used\n",
    "print(rag_chain.llm)\n",
    "\n",
    "# Local test for RAG retriever system\n",
    "\n",
    "# Check that retriever is of expected type:\n",
    "rag_retriever = rag_chain.retriever_system\n",
    "type_test = isinstance(rag_retriever, VectorStoreRetriever)\n",
    "print('Your retriever is of type VectorStoreRetriever:', type_test)\n",
    "\n",
    "# Check that the retriever retrieves relevant chunks\n",
    "output_retrieved_chunks = rag_retriever.invoke(local_test.relevance_prompt)\n",
    "relevance_test = True\n",
    "for chunk in output_retrieved_chunks:\n",
    "    if local_test.relevant_pdf not in chunk.metadata['source']:\n",
    "        relevance_test = False\n",
    "print('Your retriever returns chunks from the expected relevant documents:', relevance_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd18732",
   "metadata": {},
   "source": [
    "### 1.2.2) Formatting the Prompt with PromptTemplates\n",
    "\n",
    "Prompts are a set of instructions that are given to an LLM in order to guide it to produce responses that are coherent, contextual and relevant. It usually takes several edits and changes to the prompt until the LLM produces the desirable response. This process is called Prompt Engineering. It is common practice to save a good, desirable prompt as a template for any time this prompt is needed to be used again. This is done with PromptTemplates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e8b31",
   "metadata": {},
   "source": [
    "The **createPrompt** function in **rag_chain.py** takes as an input parameter, a dictionary that stores a question along with 4 possible answer choices. For example, the input parameter could look like:\n",
    "\n",
    "{ <br>\n",
    "   &emsp; 'question': \"What is the main contribution of the Transformer architecture?\", <br>\n",
    "   &emsp; 'A': \"It introduces convolutional layers for sequence tasks.\", <br>\n",
    "   &emsp; 'B': \"It improves word embeddings using context.\", <br>\n",
    "   &emsp; 'C': \"It removes recurrence and uses self-attention mechanisms.\", <br>\n",
    "   &emsp; 'D': \"It uses RNNs for language modeling.\" <br>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the empty prompt template\n",
    "prompt_template = rag_chain.createPrompt(question={'question': \"\", \"A\": \"\", \"B\": \"\", \"C\": \"\", \"D\": \"\"})\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424710bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the formatted prompt with the question. This is what will be passed to the LLM.\n",
    "from local_tests.rag_test import RAG_Test\n",
    "tests = RAG_Test()\n",
    "\n",
    "prompt_with_question = rag_chain.createPrompt(question=tests.question1)\n",
    "print(prompt_with_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02500eae",
   "metadata": {},
   "source": [
    "### 1.2.3) Creating Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cbe028",
   "metadata": {},
   "source": [
    "In LangChain, \"chains\" are a core concept designed to manage and streamline interactions with language models. They allow creating sequences of operations where the output of one step can be used as the input for the next. This is particularly useful for building complex workflows and applications that involve multiple stages of processing.\n",
    "\n",
    "Langchain offers a RetrievalQA chain, which combines the retriever module with a QA chain (short for Question-Answering). The retriever is used to retrieve relevant documents from the vectorstore, and the QA chain answers questions based on the retrieved documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ccac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = rag_chain.createRAGChain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e32d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_tests.rag_test import RAG_Test\n",
    "\n",
    "tests = RAG_Test()\n",
    "\n",
    "print('Local Tests for end-to-end RAG pipeline', '\\n\\n')\n",
    "\n",
    "questions = tests.local_test_questions\n",
    "answers = tests.local_test_answers\n",
    "\n",
    "correct = 0\n",
    "total = len(questions)\n",
    "for i in range(len(questions)):\n",
    "    # Create the prompt with a question\n",
    "    prompt_with_question = rag_chain.createPrompt(question=questions[i])\n",
    "    print(prompt_with_question)\n",
    "\n",
    "    # Query the LLM\n",
    "    response = qa_chain(prompt_with_question)\n",
    "\n",
    "    print('Answer selected by the LLM:', response['result'])\n",
    "    print('Correct answer:', answers[i])\n",
    "\n",
    "    if response['result'] == answers[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(f'{correct}/{total} questions answered correctly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6873ad",
   "metadata": {},
   "source": [
    "## 2) Hosting and Deploying LLM and RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e77859",
   "metadata": {},
   "source": [
    "This section will explore Ollama-hosted and Flask Ollama-hosted LLMs within the RAG pipeline created in the previous question. The RAG system will be deployed using a Flask container, simulating a real-world LLM Pipeline deployment.\n",
    "\n",
    "This section will compare different hosting methods:\n",
    "\n",
    "- **Cloud-based LLMs** (Hugging Face Hub or Spaces) offload the burden of compute but require external API calls.\n",
    "- **Local LLMs** (Ollama) offer greater control over data, cost, and customizations, but compute is limited by hardware.\n",
    "- **Network-hosted LLMs** (Flask Ollama) allow for remote access within a private infrastructure, which is useful for on-premise deployments in industries like healthcare and finance where data privacy is critical.\n",
    "  \n",
    "In many real-world applications, LLMs are accessed through APIs rather than used directly. In section 1), Hugging Faceâ€™s API was used via Hugging Face Spaces. Now, the same RAG pipeline will be deployed but using Ollama and Flask.\n",
    "\n",
    "The approach presented here is only one way to accomplish the task, but serves as an introduction and a vital opportunity to experiment with different deployment strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa398706",
   "metadata": {},
   "source": [
    "## 2.1) Using Ollama with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35563690",
   "metadata": {},
   "source": [
    "This section introduces Ollama and use the Ollama LLM in the RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce07f814",
   "metadata": {},
   "source": [
    "### 2.1.1) Getting Started with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5db71",
   "metadata": {},
   "source": [
    "First, [Ollama](https://ollama.com/) needs to be downloaded and the following model will need to be pulled:\n",
    "- `llama3.2`\n",
    "  \n",
    "Refer to [ollama-python Documentation](https://github.com/ollama/ollama-python) (prerequisites section) and [Quickstart Guide](https://github.com/ollama/ollama/blob/main/README.md#quickstart) as necessary. Make sure the ollama server is running on the machine prior to running the local test below.\n",
    "\n",
    "**Ollama must be kept running for the next sections to work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052488e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@abonia/running-ollama-in-google-colab-free-tier-545609258453\n",
    "# !ollama run gemma3\n",
    "!pip install colab-xterm\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb1cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%xterm\n",
    "\n",
    "# curl https://ollama.ai/install.sh | sh\n",
    "# ollama serve &        # start the server\n",
    "# ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca88cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=(true | false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafff2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pipeline as tap\n",
    "from local_tests.deploy_test import Deploy_Test\n",
    "\n",
    "local_test = Deploy_Test()\n",
    "\n",
    "# Check that Ollama returns the expected response\n",
    "response = tap.query_ollama(local_test.ollama_query)\n",
    "response_check = response == local_test.ollama_response\n",
    "print('Your Ollama server returned the expected response:', response_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8db7338",
   "metadata": {},
   "source": [
    "### 2.1.2) Using Ollama with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e34d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_tests.deploy_test import Deploy_Test\n",
    "from rag_chain import RAG_Chain\n",
    "\n",
    "# Local test for RAG using Ollama LLM\n",
    "\n",
    "# Check that Ollama works with rag_chain.py\n",
    "rag_chain_oo = RAG_Chain(data_dir='./data/papers/', llm_type=\"ollama_only\", init_retriever=False)\n",
    "rag_chain_oo.llm.temperature = 0\n",
    "local_test = Deploy_Test()\n",
    "\n",
    "# Printing the LLM used\n",
    "print(f\"LLM Info:\\n {rag_chain_oo.llm}\\n\")\n",
    "\n",
    "# Check that RAG returns the expected response\n",
    "response = rag_chain_oo.query_the_llm(local_test.ollama_rag_query)\n",
    "response_check = response == local_test.ollama_rag_response\n",
    "print('Your Ollama LLM in the RAG system returned the expected response:', response_check)\n",
    "print(\"\\nQUERY:\", local_test.ollama_rag_query)\n",
    "print(\"EXPECTED RESPONSE:\", local_test.ollama_rag_response)\n",
    "print(f\"Your Response: {response}\" if not response_check else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947fd6b0",
   "metadata": {},
   "source": [
    "## 2.2) Deploying an LLM and using it with RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e498c0",
   "metadata": {},
   "source": [
    "In the next steps, the Ollama LLM from 2.1.1) will be deployed by containerizing the LLM using Flask. While Ollama works locally, Flask can expose the LLM for network use. It is possible to query into this Flask Ollama LLM by reusing Langchain's OpenAI wrapper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e36fb05",
   "metadata": {},
   "source": [
    "### 2.2.1) Using Flask to Containerize the Ollama LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a4fbad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
